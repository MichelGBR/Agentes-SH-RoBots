# ğŸ¤– Sobre o projeto :

A proposta geral do projeto Ã© utilizar InteligÃªncia Artificial e automaÃ§Ãµes desenvolvidas em Python para otimizar processos internos, reduzir tarefas manuais e aumentar a eficiÃªncia operacional. JÃ¡ foram implementadas diversas automaÃ§Ãµes que eliminaram mais de 5 horas de trabalho repetitivo, tornando os fluxos atÃ© 6 vezes mais rÃ¡pidos. Essa iniciativa marca o inÃ­cio de uma jornada contÃ­nua de melhorias, com foco em escalabilidade, agilidade e geraÃ§Ã£o de valor para o negÃ³cio.

<div align="left" >

# ğŸ§©â€‹ Ferramenta utilizada:


<img align="left" height="90" width="80" src="https://github.com/user-attachments/assets/352a6e03-ecf0-47ed-a8a7-0faead584435">

<br>
<br> 
<br>
<br> 

# ğŸ’» Bibliotecas usadas nos Agentes:

Streamlit â€“ Para a interface web simples e funcional

LangChain â€“ Para a orquestraÃ§Ã£o da cadeia de recuperaÃ§Ã£o e geraÃ§Ã£o

FAISS â€“ Para a indexaÃ§Ã£o e busca de vetores

Hugging Face Embeddings â€“ Para transformar o conteÃºdo textual em vetores

Ollama â€“ Para rodar o modelo LLM localmente (ex: LLaMA 2)

# ğŸ§  Processo

1. Leitura do arquivo: O script carrega um arquivo .txt com blocos de informaÃ§Ãµes sobre mim.

2. CriaÃ§Ã£o do vetor semÃ¢ntico: Cada trecho do texto Ã© transformado em vetores com o modelo all-MiniLM-L6-v2.

3. IndexaÃ§Ã£o com FAISS: Os vetores sÃ£o indexados para rÃ¡pida recuperaÃ§Ã£o.

4. RAG (GeraÃ§Ã£o Aumentada por RecuperaÃ§Ã£o): Ao digitar uma pergunta, o sistema busca os trechos mais relevantes do texto e passa essas informaÃ§Ãµes ao modelo de linguagem (LLM), que entÃ£o responde com base apenas nesses dados.

5. Interface: Tudo isso Ã© disponibilizado por uma interface amigÃ¡vel via Streamlit.
